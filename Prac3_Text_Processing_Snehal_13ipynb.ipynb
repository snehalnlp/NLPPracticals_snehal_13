{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prac3_Text_Processing_Snehal_13ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehalnlp/NLPPracticals_snehal_13/blob/main/Prac3_Text_Processing_Snehal_13ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgyvFJRi5rBr"
      },
      "source": [
        "Text Processing - 1\n",
        "\n",
        "Tokenize Text using NLTK                                              \n",
        "Sentence Tokenization                                                              \n",
        "PunktSentenceTokenizer                                                          Tokenize sentence of different      language                                            \n",
        "Word Tokenization                           \n",
        "Using TreebankWordTokenizer                                                      \n",
        "PunktWordTokenizer                                                              \n",
        "WordPunctTokenizer                                                              \n",
        "Using Regular Expression                                                      \n",
        "Code #1: Tokenize text using NLTK in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYagcCng6bUF",
        "outputId": "54cdf3f4-b3ed-4154-a119-f0810c61f5b0"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from urllib import request\n",
        "from __future__ import division\n",
        "import nltk, re, pprint\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2SUHUXf7NMK"
      },
      "source": [
        "text = \"Natural language processing (NLP) is a field \" + \\\n",
        "       \"of computer science, artificial intelligence \" + \\\n",
        "       \"and computational linguistics concerned with \" + \\\n",
        "       \"the interactions between computers and human \" + \\\n",
        "       \"(natural) languages, and, in particular, \" + \\\n",
        "       \"concerned with programming computers to \" + \\\n",
        "       \"fruitfully process large natural language \" + \\\n",
        "       \"corpora. Challenges in natural language \" + \\\n",
        "       \"processing frequently involve natural \" + \\\n",
        "       \"language understanding, natural language\" + \\\n",
        "       \"generation frequently from formal, machine\" + \\\n",
        "       \"-readable logical forms), connecting language \" + \\\n",
        "       \"and machine perception, managing human-\" + \\\n",
        "       \"computer dialog systems, or some combination \" + \\\n",
        "       \"thereof.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7bVibIC8b2m",
        "outputId": "a93721d9-6b98-4f92-f8b9-fd00b0434d38"
      },
      "source": [
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzgvgqyx8gtr",
        "outputId": "f7fecd7d-e2eb-4c97-ff2e-4d4c725e1a03"
      },
      "source": [
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAk8vMa8nVQ"
      },
      "source": [
        "Tokenizing text, sentence, words works\n",
        "\n",
        "Code #2: Sentence Tokenization – Splitting sentences in the paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoY_gI3G8q-K",
        "outputId": "d9659916-804d-4fba-f546-93afa7f99dc1"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "  \n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
        "sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjkHxQE9SFP"
      },
      "source": [
        "Code #3: PunktSentenceTokenizer – When we have huge chunks of data then it is efficient to use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv9h197p9XR8",
        "outputId": "f431c7ca-acfe-4583-b619-84de9a1ce108"
      },
      "source": [
        "import nltk.data\n",
        "  \n",
        "# Loading PunktSentenceTokenizer using English pickle file\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
        "  \n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk9xEYzF97da"
      },
      "source": [
        "Code #4: Tokenize sentence of different language – One can also tokenize sentence from different languages using different pickle file other than English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leAHZWdQ-AjO",
        "outputId": "902387df-a9b0-4cdb-a13a-9656c9bff7db"
      },
      "source": [
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "  \n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola amigo.', 'Estoy bien.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSQD9o9F-F8F"
      },
      "source": [
        "Code #5: Word Tokenization – Splitting words in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu_jgcfE-KgX",
        "outputId": "0f48af4f-e66b-44ce-e81e-38db8fcd6035"
      },
      "source": [
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPSzbTHa-fjh"
      },
      "source": [
        "Code #6: Using TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zxo9sU6-hmr",
        "outputId": "a54dd7d0-9959-4b43-bb1d-be4367566cba"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "  \n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca1xH4kq-qx_"
      },
      "source": [
        "Code #7: WordPunctTokenizer – It seperates the punctuation from the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edf2exvr-rxM",
        "outputId": "a30e4b3e-ab52-4179-93c7-8430445ab6d7"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(\"Let's see how it's working.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C86ek2A5_QQv"
      },
      "source": [
        "Code #8: Using Regular Expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVwzdLEp_Q_5",
        "outputId": "ab9add2e-c38f-4b05-ccb2-ca11b92e6bab"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "  \n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "text = \"Let's see how it's working.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'see', 'how', \"it's\", 'working']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO09Amxm_fAS"
      },
      "source": [
        "Code #9: Using Regular Expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIPL2yUD_hAa",
        "outputId": "ac4768fa-0a5f-4752-b9cd-050ea7aa61d8"
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "  \n",
        "text = \"Let's see how it's working.\"\n",
        "regexp_tokenize(text, \"[\\w']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'see', 'how', \"it's\", 'working']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}